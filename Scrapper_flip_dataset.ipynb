{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lw8ru8qgUwo"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "from random import random\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import urllib.parse as urlparse\n",
        "from urllib.parse import parse_qs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "BASE_URL = 'https://www.flipkart.com/'\n",
        "SEARCH_QUERY = \"headphones\"\n",
        "TOP_N_PRODUCTS = 10\n",
        "REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT = 100 #10 Reviews exist per page"
      ],
      "metadata": {
        "id": "4R8Qg77wgenX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_URL = \"https://www.flipkart.com/boat-rockerz-400-bluetooth-headset/product-reviews/itm14d0416b87d55?pid=ACCEJZXYKSG2T9GS&lid=LSTACCEJZXYKSG2T9GSVY4ZIC&marketplace=FLIPKART&page=1\"\n",
        "r = requests.get(SAMPLE_URL)\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "print(soup.prettify()[:500])"
      ],
      "metadata": {
        "id": "IWyGTm7Jgh2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rows = soup.find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
        "print(f\"Count of rows(reviews):{len(rows)}\\n\\n\\n\")\n",
        "# iteration over all blocks\n",
        "for row in rows:\n",
        "    # Print a sample row(review html block)\n",
        "    # print(f\"row:\\n{row} \\n\\n\")\n",
        "\n",
        "    # finding all rows within the block\n",
        "    sub_row = row.find_all('div',attrs={'class':'row'})\n",
        "\n",
        "    # extracting text from 1st and 2nd row\n",
        "    rating = sub_row[0].find('div').text\n",
        "    print(f\"rating:{rating} \\n\\n\")\n",
        "\n",
        "    summary = sub_row[0].find('p').text\n",
        "    print(f\"summary:{summary} \\n\\n\")\n",
        "\n",
        "    review = sub_row[1].find_all('div')[2].text\n",
        "    print(f\"review:{review} \\n\\n\")\n",
        "\n",
        "    location = sub_row[3].find('p',attrs={'class':'_2mcZGG'}).find_all('span')[1].text\n",
        "    location = \"\".join(location.split(\",\")[1:]).strip()\n",
        "    print(f\"location:{location} \\n\\n\")\n",
        "\n",
        "    date = sub_row[3].find_all('p',attrs={'class':'_2sc7ZR'})[1].text\n",
        "    print(f\"date:{date} \\n\\n\")\n",
        "\n",
        "\n",
        "    sub_row_2 = row.find_all('div',attrs={'class':'_1e9_Zu'})[0].find_all('span',attrs={'class':'_3c3Px5'})\n",
        "\n",
        "    upvotes = sub_row_2[0].text\n",
        "    print(f\"upvotes:{upvotes} \\n\\n\")\n",
        "\n",
        "    downvotes = sub_row_2[1].text\n",
        "    print(f\"downvotes:{downvotes} \\n\\n\")\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "PbyM4FsWgk5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_popular_product_s_titles_and_urls(search_query : str, popular_products_count_limit : int = None):\n",
        "\n",
        "    search_url = f\"{BASE_URL}search?q={search_query}&sort=popularity\"\n",
        "    search_response = requests.get(search_url)\n",
        "\n",
        "    # Pause the loop for 1-3 seconds to simulate natural setting not overwhelm the server with back to back requests without any pause\n",
        "    # sleep(randint(1,3))\n",
        "\n",
        "    search_html_soup = BeautifulSoup(search_response.content, 'html.parser')\n",
        "    search_results_products = search_html_soup.find_all('div',attrs={'class':'_4ddWXP'})\n",
        "\n",
        "    product_titles, product_urls = [],[]\n",
        "\n",
        "    product_count = 0\n",
        "\n",
        "    for product in tqdm(search_results_products, desc=\"Search Results Iteration\", position=0, leave=True):\n",
        "\n",
        "        ad_mention_subrow = product.find(\"div\", attrs={\"class\":\"_4HTuuX\"})\n",
        "\n",
        "        is_ad = not not ad_mention_subrow\n",
        "\n",
        "        if not is_ad:\n",
        "\n",
        "            title_mention_subrow = product.find(\"a\", attrs={\"class\":\"s1Q9rs\"})\n",
        "\n",
        "            product_title = title_mention_subrow[\"title\"]\n",
        "            product_relative_url = title_mention_subrow[\"href\"]\n",
        "            product_url = urljoin(BASE_URL,product_relative_url)\n",
        "\n",
        "            parsed_url = urlparse.urlparse(product_url)\n",
        "            parsed_url_path = parsed_url.path\n",
        "            parsed_url_path_split = parsed_url_path.split(\"/\")\n",
        "            parsed_url_path_split[2] = \"product-reviews\"\n",
        "            parsed_url_path_modified = \"/\".join(parsed_url_path_split)\n",
        "            parsed_url_modified = parsed_url._replace(path=parsed_url_path_modified)\n",
        "            product_url = parsed_url_modified.geturl()\n",
        "\n",
        "            product_titles.append(product_title)\n",
        "            product_urls.append(product_url)\n",
        "\n",
        "            product_count += 1\n",
        "\n",
        "            if popular_products_count_limit and (product_count >= popular_products_count_limit):\n",
        "                break\n",
        "\n",
        "    return product_titles, product_urls"
      ],
      "metadata": {
        "id": "XLpjLo3ggvks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_titles, product_urls = get_popular_product_s_titles_and_urls(SEARCH_QUERY, TOP_N_PRODUCTS);"
      ],
      "metadata": {
        "id": "m7_OPvaNg292"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"# Products\", \"# Reviews Per Page\", \"# Pages\", \"# Total Reviews Count\"]\n",
        "x.add_row([len(product_urls), 10, REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT, len(product_urls)*10*REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "GCeqBIvqg7cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for idx, url in enumerate(tqdm(product_urls, desc='products')):\n",
        "    # iterating over review pages\n",
        "    for i in tqdm(range(1,REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT+1), desc=\"review pages\", position=0, leave=False):\n",
        "        parsed = urlparse.urlparse(url)\n",
        "        pid = parse_qs(parsed.query)['pid'][0]\n",
        "        URL = f\"{url}&page={i}\"\n",
        "\n",
        "        r = requests.get(URL)\n",
        "\n",
        "        # Pause the loop for 0-1 seconds to simulate natural setting not overwhelm the server with back to back requests without any pause\n",
        "        sleep(random())\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "        rows = soup.find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
        "\n",
        "        for row in rows:\n",
        "\n",
        "            # finding all rows within the block\n",
        "            sub_row = row.find_all('div',attrs={'class':'row'})\n",
        "\n",
        "            # extracting text from 1st 2nd and 4th row\n",
        "            rating = sub_row[0].find('div').text\n",
        "            summary = sub_row[0].find('p').text\n",
        "            summary = summary.strip()\n",
        "            review = sub_row[1].find_all('div')[2].text\n",
        "            review = review.strip()\n",
        "            location=\"\"\n",
        "            location_row = sub_row[3].find('p',attrs={'class':'_2mcZGG'})\n",
        "            if location_row:\n",
        "                location_row = location_row.find_all('span')\n",
        "                if len(location_row)>=2:\n",
        "                    location = location_row[1].text\n",
        "                    location = \"\".join(location.split(\",\")[1:]).strip()\n",
        "            date = sub_row[3].find_all('p',attrs={'class':'_2sc7ZR'})[1].text\n",
        "\n",
        "            sub_row_2 = row.find_all('div',attrs={'class':'_1e9_Zu'})[0].find_all('span',attrs={'class':'_3c3Px5'})\n",
        "\n",
        "            upvotes = sub_row_2[0].text\n",
        "            downvotes = sub_row_2[1].text\n",
        "\n",
        "            # appending to data\n",
        "            dataset.append({'product_id':pid, 'product_title':product_titles[idx], 'rating': rating, 'summary': summary, 'review': review, 'location' : location, 'date' : date, 'upvotes' : upvotes, 'downvotes' : downvotes})"
      ],
      "metadata": {
        "id": "GmENq58fg-Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "with pd.option_context('display.max_colwidth', -1):\n",
        "    display(df.head(5))\n",
        "    display(df.tail(5))"
      ],
      "metadata": {
        "id": "gA8LrZbXhIYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_reviews = df.shape[0]\n",
        "print(f\"Count of reviews:{count_reviews}\")"
      ],
      "metadata": {
        "id": "h-5fir0_hPIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"./flipkart_reviews_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "Sk8oACGrhReF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}